{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0su8MXlnoPJ"
   },
   "source": [
    "## Elo Merchant Category Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMAMsrYnYWKo"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Ukpd1E_Z_uV"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "gc.collect()\n",
    "import pickle\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, date\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3d-uo3gdaC4e"
   },
   "outputs": [],
   "source": [
    "#Add All the Models Libraries\n",
    "\n",
    "# Scalers\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Models\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Common data processors\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vys4R3qtreAR"
   },
   "source": [
    "### Loading data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "DSpWfIqd68NE",
    "outputId": "8e1d31b9-514c-40ea-bff5-8b989f3e4a75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "r04jVhYqZCiG",
    "outputId": "d30c664a-d25e-4fcd-96f0-ebc4a5e7bb22"
   },
   "outputs": [],
   "source": [
    "#Using Kaggle datasets in Colab\n",
    "# Run this cell and select the kaggle.json file downloaded\n",
    "# from the Kaggle account settings page.\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "# Let's make sure the kaggle.json file is present.\n",
    "!ls -lha kaggle.json\n",
    "# Next, install the Kaggle API client.\n",
    "!pip install -q kaggle\n",
    "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
    "# so move it there.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# This permissions change avoids a warning on Kaggle tool startup.\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "sNJdszV5Zw7v",
    "outputId": "ef02b1f1-2fe8-4f0b-e15f-b53111a6f043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "Downloading sample_submission.csv.zip to /content\n",
      "  0% 0.00/846k [00:00<?, ?B/s]\n",
      "100% 846k/846k [00:00<00:00, 58.0MB/s]\n",
      "Downloading new_merchant_transactions.csv.zip to /content\n",
      " 83% 41.0M/49.4M [00:00<00:00, 82.0MB/s]\n",
      "100% 49.4M/49.4M [00:00<00:00, 111MB/s] \n",
      "Downloading train.csv.zip to /content\n",
      "  0% 0.00/3.02M [00:00<?, ?B/s]\n",
      "100% 3.02M/3.02M [00:00<00:00, 100MB/s]\n",
      "Downloading Data%20Dictionary.xlsx to /content\n",
      "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
      "100% 17.2k/17.2k [00:00<00:00, 17.8MB/s]\n",
      "Downloading historical_transactions.csv.zip to /content\n",
      " 98% 539M/548M [00:04<00:00, 115MB/s]\n",
      "100% 548M/548M [00:04<00:00, 127MB/s]\n",
      "Downloading test.csv.zip to /content\n",
      "  0% 0.00/1.13M [00:00<?, ?B/s]\n",
      "100% 1.13M/1.13M [00:00<00:00, 165MB/s]\n",
      "Downloading Data_Dictionary.xlsx to /content\n",
      "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
      "100% 17.2k/17.2k [00:00<00:00, 17.9MB/s]\n",
      "Downloading merchants.csv.zip to /content\n",
      " 39% 5.00M/12.7M [00:00<00:00, 40.6MB/s]\n",
      "100% 12.7M/12.7M [00:00<00:00, 61.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Copy the stackoverflow data set locally.\n",
    "!kaggle competitions download -c elo-merchant-category-recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TAus2LrOZ3MN"
   },
   "outputs": [],
   "source": [
    "#Extract zip files\n",
    "import zipfile\n",
    "for file in ['/content/sample_submission.csv.zip','/content/historical_transactions.csv.zip','/content/train.csv.zip','/content/test.csv.zip', '/content/new_merchant_transactions.csv.zip', '/content/merchants.csv.zip' ]:\n",
    "    zip_ref = zipfile.ZipFile(file, 'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnE_-UiqnoWI"
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OLSCSwXo4Rc9"
   },
   "outputs": [],
   "source": [
    "#Reduce the memory usage - Inspired by Panchajanya Banerjee\n",
    "#determine and apply the smallest data type that can fit the range of values\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    '''\n",
    "    Reduce the memory usage by applying the smallest data type that can fit the range of values\n",
    "    '''\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vNCKLgqxxPe"
   },
   "outputs": [],
   "source": [
    "def aggregate_transaction_hist(trans, prefix):  \n",
    "    '''\n",
    "    Create aggrgate features from all hitorical transaction features\n",
    "    '''\n",
    "    agg_func = {\n",
    "          'purchase_amount' : ['sum','max','min','mean','var','skew'],\n",
    "          'installments' : ['sum','max','mean','var','skew'],\n",
    "          'purchase_date' : ['max','min'],\n",
    "          'month_lag' : ['max','min','mean','var','skew'],\n",
    "          'month_diff' : ['max','min','mean','var','skew'],\n",
    "          'weekend' : ['sum', 'mean'],\n",
    "          'weekday' : ['sum', 'mean'],\n",
    "          'authorized_flag': ['sum', 'mean'],\n",
    "          'category_1': ['sum','mean', 'max','min'],\n",
    "          'card_id' : ['size','count'],\n",
    "          'month': ['nunique', 'mean', 'min', 'max'],\n",
    "          'hour': ['nunique', 'mean', 'min', 'max'],\n",
    "          'weekofyear': ['nunique', 'mean', 'min', 'max'],\n",
    "          'day': ['nunique', 'mean', 'min', 'max'],\n",
    "          'subsector_id': ['nunique'],\n",
    "          'merchant_id': ['nunique'],\n",
    "          'merchant_category_id' : ['nunique'],\n",
    "          'price' :['sum','mean','max','min','var'],\n",
    "          'duration' : ['mean','min','max','var','skew'],\n",
    "          'amount_month_ratio':['mean','min','max','var','skew']\n",
    "        \n",
    "      }\n",
    "    \n",
    "    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + '_'.join(col).strip() \n",
    "                           for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    \n",
    "    df = (trans.groupby('card_id')\n",
    "            .size()\n",
    "            .reset_index(name='{}transactions_count'.format(prefix)))\n",
    "    \n",
    "    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "    \n",
    "    return agg_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFNHJDG2yIOY"
   },
   "outputs": [],
   "source": [
    "def aggregate_transaction_new(trans, prefix):  \n",
    "    '''\n",
    "    Create aggrgate features from all new_transactions features\n",
    "    '''        \n",
    "    agg_func = {\n",
    "          'purchase_amount' : ['sum','max','min','mean','var','skew'],\n",
    "          'installments' : ['sum','max','mean','var','skew'],\n",
    "          'purchase_date' : ['max','min'],\n",
    "          'month_lag' : ['max','min','mean','var','skew'],\n",
    "          'month_diff' : ['max','min','mean','var','skew'],\n",
    "          'weekend' : ['sum', 'mean'],\n",
    "          'weekday' : ['sum', 'mean'],\n",
    "          'authorized_flag': ['sum', 'mean'],\n",
    "          'category_1': ['sum','mean', 'max','min'],\n",
    "          'card_id' : ['size','count'],\n",
    "          'month': ['nunique', 'mean', 'min', 'max'],\n",
    "          'hour': ['nunique', 'mean', 'min', 'max'],\n",
    "          'weekofyear': ['nunique', 'mean', 'min', 'max'],\n",
    "          'day': ['nunique', 'mean', 'min', 'max'],\n",
    "          'subsector_id': ['nunique'],\n",
    "          'merchant_category_id' : ['nunique'],\n",
    "          'price' :['sum','mean','max','min','var'],\n",
    "          'duration' : ['mean','min','max','var','skew'],\n",
    "          'amount_month_ratio':['mean','min','max','var','skew']\n",
    "    }\n",
    "    \n",
    "    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + '_'.join(col).strip() \n",
    "                           for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    \n",
    "    df = (trans.groupby('card_id')\n",
    "            .size()\n",
    "            .reset_index(name='{}transactions_count'.format(prefix)))\n",
    "    \n",
    "    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "    \n",
    "    return agg_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HikKv54f16AC"
   },
   "outputs": [],
   "source": [
    "def merchants_agg(merchants):\n",
    "    '''\n",
    "    Aggrgating with Mean and mode for various features\n",
    "    '''\n",
    "    mode2 = lambda x: Counter(x).most_common(1)[0][0]\n",
    "    merchants = merchants.groupby(\"merchant_id\",as_index=False).agg({\n",
    "        \"merchant_group_id\": mode2,\n",
    "        \"merchant_category_id\": mode2,\n",
    "        \"subsector_id\": mode2,\n",
    "        \"numerical_1\": \"mean\",\n",
    "        \"numerical_2\": \"mean\",\n",
    "        \"category_1\": mode2,\n",
    "        \"most_recent_sales_range\": mode2,\n",
    "        \"most_recent_purchases_range\": mode2,\n",
    "        \"avg_sales_lag3\": \"mean\",\n",
    "        \"avg_purchases_lag3\": \"mean\",\n",
    "        \"active_months_lag3\": mode2,\n",
    "        \"avg_sales_lag6\": \"mean\",\n",
    "        \"avg_purchases_lag6\": \"mean\",\n",
    "        \"active_months_lag6\": mode2,\n",
    "        \"avg_sales_lag12\": \"mean\",\n",
    "        \"avg_purchases_lag12\": \"mean\",\n",
    "        \"active_months_lag12\": mode2,\n",
    "        \"category_4\": mode2,\n",
    "        \"city_id\": mode2,\n",
    "        \"state_id\": mode2,\n",
    "        \"category_2\": mode2\n",
    "    })\n",
    "\n",
    "    return merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCDVd2cY05O8"
   },
   "outputs": [],
   "source": [
    "#Aggregating by card_id since there are duplicated card_id's:\n",
    "#Numerical features: mean (without missings)\n",
    "#Non-numerical features: mode\n",
    "def new_transactions_agg(new_transactions):\n",
    "    '''\n",
    "    Aggrgating with Mean and mode for various features\n",
    "    '''\n",
    "    mode2 = lambda x: Counter(x).most_common(1)[0][0]\n",
    "    new_transactions = new_transactions.groupby(\"card_id\",as_index=False).agg({\n",
    "        \"authorized_flag\": mode2,\n",
    "        \"merchant_category_id\": mode2,\n",
    "        \"subsector_id\": mode2,\n",
    "        \"merchant_id\":  mode2,\n",
    "        \"category_1\": mode2,\n",
    "        'month_lag': mode2,\n",
    "        \"installments\": mode2,\n",
    "        \"purchase_amount\": \"mean\",\n",
    "        \"city_id\": mode2,\n",
    "        \"state_id\": mode2,\n",
    "        \"category_2\": mode2,\n",
    "        \"purchase_date\": mode2\n",
    "    })\n",
    "    return new_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzsCvxah_diS"
   },
   "outputs": [],
   "source": [
    "#Aggregating by card_id since there are duplicated card_id's:\n",
    "def mean_encode(train, new_transactions, merchants, suffix):\n",
    "    '''\n",
    "    target Mean encoding of categorical features\n",
    "    '''\n",
    "    new_transactions = new_transactions_agg(new_transactions)\n",
    "    new_transactions.columns = [col+suffix if col not in [\"card_id\", \"merchant_id\"] else col for col in new_transactions.columns]\n",
    "\n",
    "    # Select only train card_id's\n",
    "    train_new_trans = pd.merge(train['card_id'], new_transactions, on='card_id',how='left')\n",
    "    del new_transactions\n",
    "\n",
    "    # Similarly Aggregating by merchant_id since there are some duplicated merchants_id with different features values:\n",
    "    merchants = merchants_agg(merchants)\n",
    "\n",
    "    #Adding Merchants features to train data\n",
    "    merchants.columns = [col+\"_merchants\"+suffix if col!=\"merchant_id\" else col for col in merchants.columns]\n",
    "    all_transactions_merchants = pd.merge(train_new_trans, merchants, on='merchant_id',how='left')\n",
    "    del merchants\n",
    "    del train_new_trans\n",
    "    del all_transactions_merchants['merchant_id']\n",
    "\n",
    "    card_ids_train = train[\"card_id\"].unique()\n",
    "    idxs = all_transactions_merchants[\"card_id\"].isin(list(card_ids_train))\n",
    "    all_transactions_merchants_filter = all_transactions_merchants.loc[idxs]\n",
    "    all_transactions_merchants_filter_target = all_transactions_merchants_filter.merge(train[[\"card_id\",\\\n",
    "                                           \"target\"]],how=\"left\",left_on=\"card_id\",right_on=\"card_id\")\n",
    "    del all_transactions_merchants_filter\n",
    "    gc.collect()\n",
    "\n",
    "    all_transactions_merchants.fillna(-9999,inplace=True)\n",
    "\n",
    "\n",
    "    # Mean encoding\n",
    "    features_mean_encoding = ['authorized_flag'+suffix, 'category_1'+suffix, 'month_lag'+suffix, 'installments'+suffix,'category_2'+suffix,\n",
    "        'numerical_1_merchants'+suffix, 'category_1_merchants'+suffix, 'most_recent_sales_range_merchants'+suffix,\n",
    "        'most_recent_purchases_range_merchants'+suffix, 'active_months_lag3_merchants'+suffix, \n",
    "        'active_months_lag6_merchants'+suffix, 'active_months_lag12_merchants'+suffix, 'category_4_merchants'+suffix,\n",
    "        'category_2_merchants'+suffix]\n",
    "\n",
    "    for col in features_mean_encoding:\n",
    "        mean_encoding = all_transactions_merchants_filter_target.groupby(col).target.mean()\n",
    "        all_transactions_merchants[col+\"_mean_encoded\"] = all_transactions_merchants[col].\\\n",
    "                                                                 map(mean_encoding)\n",
    "    del mean_encoding\n",
    "    gc.collect()   \n",
    "    return all_transactions_merchants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eY6TyoV6lajs"
   },
   "source": [
    "#### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAWAr8ZgZeIZ"
   },
   "outputs": [],
   "source": [
    "def getfeatures_train(train, transactions = None, new_transactions = None, merchants = None):\n",
    "  # create Outliers feature \n",
    "  '''\n",
    "  create train features from historical, new and merchants trancactions\n",
    "  '''\n",
    "  train['outliers'] = 0\n",
    "  train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "  train['outliers'].value_counts()\n",
    "  # Mean encoding features with outlier feature\n",
    "  for features in ['feature_1','feature_2','feature_3']:\n",
    "      order_label = train.groupby([features])['outliers'].mean()\n",
    "      train[features+\"_mean\"] = train[features].map(order_label)\n",
    "      \n",
    "  # Now extract the days and Qtr\n",
    "  train['days'] = (date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\n",
    "  train['quarter'] = train['first_active_month'].dt.quarter\n",
    "\n",
    "  feature_cols = ['feature_1', 'feature_2', 'feature_3']\n",
    "  for f in feature_cols:\n",
    "      train['days_' + f] = train['days'] * train[f]\n",
    "      train['days_' + f + '_ratio'] = train[f] / train['days']\n",
    "\n",
    "  gc.collect()\n",
    "  \n",
    "  # Loading historical_transactions data\n",
    "  transactions = reduce_mem_usage(pd.read_csv('../content/historical_transactions.csv'))\n",
    "  transactions.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "  print('Preprocessing hist_trans...')\n",
    "  # Feature extraction historical transaction\n",
    "  #impute missing values.\n",
    "  transactions['category_2'] = transactions['category_2'].fillna(-9999,inplace=True)\n",
    "  transactions['category_3'] = transactions['category_3'].fillna('-9999',inplace=True)\n",
    "  transactions['merchant_id'] = transactions['merchant_id'].fillna('-9999',inplace=True)\n",
    "  transactions['installments'].replace(-1, np.nan,inplace=True)\n",
    "  transactions['installments'].replace(999, np.nan,inplace=True)\n",
    "  transactions['purchase_amount'] = transactions['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "  # Feature engineering\n",
    "  transactions['authorized_flag'] = transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "  transactions['category_1'] = transactions['category_1'].map({'Y': 1, 'N': 0})\n",
    "  transactions['category_3'] = transactions['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "  transactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\n",
    "  transactions['weekofyear'] = transactions['purchase_date'].dt.weekofyear\n",
    "  transactions['month'] = transactions['purchase_date'].dt.month\n",
    "  transactions['day'] = transactions['purchase_date'].dt.day\n",
    "  transactions['weekday'] = transactions.purchase_date.dt.weekday\n",
    "  transactions['weekend'] = (transactions.purchase_date.dt.weekday >=5).astype(int)\n",
    "  transactions['hour'] = transactions['purchase_date'].dt.hour \n",
    "  transactions['month_diff'] = ((datetime.today() - transactions['purchase_date']).dt.days)//30\n",
    "  transactions['month_diff'] += transactions['month_lag']\n",
    "  transactions['duration'] = transactions['purchase_amount']*transactions['month_diff']\n",
    "  transactions['amount_month_ratio'] = transactions['purchase_amount']/transactions['month_diff']\n",
    "  transactions['price'] = transactions['purchase_amount'] / transactions['installments']\n",
    "\n",
    "  gc.collect()\n",
    "\n",
    "  agg_func = {\n",
    "          'mean': ['mean'],\n",
    "     }\n",
    "  for col in ['category_2','category_3']:\n",
    "      transactions[col+'_mean'] = transactions['purchase_amount'].groupby(transactions[col]).agg('mean')\n",
    "      transactions[col+'_max'] = transactions['purchase_amount'].groupby(transactions[col]).agg('max')\n",
    "      transactions[col+'_min'] = transactions['purchase_amount'].groupby(transactions[col]).agg('min')\n",
    "      transactions[col+'_sum'] = transactions['purchase_amount'].groupby(transactions[col]).agg('sum')\n",
    "      agg_func[col+'_mean'] = ['mean']\n",
    "    \n",
    "  gc.collect()\n",
    "\n",
    "  # Create Aggregate features on historical transaction\n",
    "  merge_hist = aggregate_transaction_hist(transactions, prefix='hist_')\n",
    "  del transactions\n",
    "  gc.collect()\n",
    "  train = pd.merge(train, merge_hist, on='card_id',how='left')\n",
    "  del merge_hist\n",
    "  gc.collect()\n",
    "      \n",
    "  #Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "  train['hist_purchase_date_max'] = pd.to_datetime(train['hist_purchase_date_max'])\n",
    "  train['hist_purchase_date_min'] = pd.to_datetime(train['hist_purchase_date_min'])\n",
    "  train['hist_purchase_date_diff'] = (train['hist_purchase_date_max'] - train['hist_purchase_date_min']).dt.days\n",
    "  train['hist_purchase_date_average'] = train['hist_purchase_date_diff']/train['hist_card_id_size']\n",
    "  train['hist_purchase_date_uptonow'] = (datetime.today() - train['hist_purchase_date_max']).dt.days\n",
    "  train['hist_purchase_date_uptomin'] = (datetime.today() - train['hist_purchase_date_min']).dt.days\n",
    "  train['hist_first_buy'] = (train['hist_purchase_date_min'] - train['first_active_month']).dt.days\n",
    "  train['hist_last_buy'] = (train['hist_purchase_date_max'] - train['first_active_month']).dt.days\n",
    "\n",
    "  for feature in ['hist_purchase_date_max','hist_purchase_date_min']:\n",
    "      train[feature] = train[feature].astype(np.int64) * 1e-9\n",
    "  gc.collect()\n",
    "\n",
    "  #Loading data new_merchant_transactions\n",
    "  new_transactions = reduce_mem_usage(pd.read_csv('/content/new_merchant_transactions.csv').sort_values('card_id',ascending=False))\n",
    "  new_transactions.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "  print('Preprocessing new_trans...')\n",
    "\n",
    "  # Feature extraction new transaction\n",
    "  #impute missing values\n",
    "  new_transactions['category_2'] = new_transactions['category_2'].fillna(-9999,inplace=True)\n",
    "  new_transactions['category_3'] = new_transactions['category_3'].fillna('-9999',inplace=True)\n",
    "  new_transactions['merchant_id'] = new_transactions['merchant_id'].fillna('-9999',inplace=True)\n",
    "  new_transactions['installments'].replace(-1, np.nan,inplace=True)\n",
    "  new_transactions['installments'].replace(999, np.nan,inplace=True)\n",
    "  new_transactions['purchase_amount'] = new_transactions['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "  #Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "  new_transactions['authorized_flag'] = new_transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "  new_transactions['category_1'] = new_transactions['category_1'].map({'Y': 1, 'N': 0})\n",
    "  new_transactions['category_3'] = new_transactions['category_3'].map({'A':0, 'B':1, 'C':2}) \n",
    "\n",
    "  new_transactions['purchase_date'] = pd.to_datetime(new_transactions['purchase_date'])\n",
    "  new_transactions['month'] = new_transactions['purchase_date'].dt.month\n",
    "  new_transactions['weekofyear'] = new_transactions['purchase_date'].dt.weekofyear\n",
    "  new_transactions['day'] = new_transactions['purchase_date'].dt.day\n",
    "  new_transactions['weekday'] = new_transactions.purchase_date.dt.weekday\n",
    "  new_transactions['weekend'] = (new_transactions.purchase_date.dt.weekday >=5).astype(int)\n",
    "  new_transactions['hour'] = new_transactions['purchase_date'].dt.hour \n",
    "  new_transactions['month_diff'] = ((datetime.today() - new_transactions['purchase_date']).dt.days)//30\n",
    "  new_transactions['month_diff'] += new_transactions['month_lag']\n",
    "\n",
    "  # additional features\n",
    "  new_transactions['duration'] = new_transactions['purchase_amount']*new_transactions['month_diff']\n",
    "  new_transactions['amount_month_ratio'] = new_transactions['purchase_amount']/new_transactions['month_diff']\n",
    "  new_transactions['price'] = new_transactions['purchase_amount'] / new_transactions['installments']\n",
    "\n",
    "  aggs = {\n",
    "          'mean': ['mean'],\n",
    "      }\n",
    "\n",
    "  for col in ['category_2','category_3']:\n",
    "      new_transactions[col+'_mean'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('mean')\n",
    "      new_transactions[col+'_max'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('max')\n",
    "      new_transactions[col+'_min'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('min')\n",
    "      new_transactions[col+'_var'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('var')\n",
    "      aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "  gc.collect()\n",
    "\n",
    "  # Create Aggregate features on new transaction\n",
    "  merge_new = aggregate_transaction_new(new_transactions, prefix='new_')\n",
    "  del new_transactions\n",
    "\n",
    "  train = pd.merge(train, merge_new, on='card_id',how='left')\n",
    "  del merge_new\n",
    "  gc.collect()\n",
    "\n",
    "  #Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "  train['new_purchase_date_max'] = pd.to_datetime(train['new_purchase_date_max'])\n",
    "  train['new_purchase_date_min'] = pd.to_datetime(train['new_purchase_date_min'])\n",
    "  train['new_purchase_date_diff'] = (train['new_purchase_date_max'] - train['new_purchase_date_min']).dt.days\n",
    "  train['new_purchase_date_average'] = train['new_purchase_date_diff']/train['new_card_id_size']\n",
    "  train['new_purchase_date_uptonow'] = (datetime.today() - train['new_purchase_date_max']).dt.days\n",
    "  train['new_purchase_date_uptomin'] = (datetime.today() - train['new_purchase_date_min']).dt.days\n",
    "  train['new_first_buy'] = (train['new_purchase_date_min'] - train['first_active_month']).dt.days\n",
    "  train['new_last_buy'] = (train['new_purchase_date_max'] - train['first_active_month']).dt.days\n",
    "  for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
    "      train[feature] = train[feature].astype(np.int64) * 1e-9\n",
    "  gc.collect()\n",
    "\n",
    "  #NEW Features referred from https://www.kaggle.com/mfjwr1/simple-lightgbm-without-blending\n",
    "  train['card_id_total'] = train['new_card_id_size']+train['hist_card_id_size']\n",
    "  train['card_id_cnt_total'] = train['new_card_id_count']+train['hist_card_id_count']\n",
    "  train['card_id_cnt_ratio'] = train['new_card_id_count']/train['hist_card_id_count']\n",
    "  train['purchase_amount_total'] = train['new_purchase_amount_sum']+train['hist_purchase_amount_sum']\n",
    "  train['purchase_amount_mean'] = train['new_purchase_amount_mean']+train['hist_purchase_amount_mean']\n",
    "  train['purchase_amount_max'] = train['new_purchase_amount_max']+train['hist_purchase_amount_max']\n",
    "  train['purchase_amount_min'] = train['new_purchase_amount_min']+train['hist_purchase_amount_min']\n",
    "  train['purchase_amount_ratio'] = train['new_purchase_amount_sum']/train['hist_purchase_amount_sum']\n",
    "  train['month_diff_mean'] = train['new_month_diff_mean']+train['hist_month_diff_mean']\n",
    "  train['month_diff_ratio'] = train['new_month_diff_mean']/train['hist_month_diff_mean']\n",
    "  train['month_lag_mean'] = train['new_month_lag_mean']+train['hist_month_lag_mean']\n",
    "  train['month_lag_max'] = train['new_month_lag_max']+train['hist_month_lag_max']\n",
    "  train['month_lag_min'] = train['new_month_lag_min']+train['hist_month_lag_min']\n",
    "  train['category_1_mean'] = train['new_category_1_mean']+train['hist_category_1_mean']\n",
    "  train['installments_total'] = train['new_installments_sum']+train['hist_installments_sum']\n",
    "  train['installments_mean'] = train['new_installments_mean']+train['hist_installments_mean']\n",
    "  train['installments_max'] = train['new_installments_max']+train['hist_installments_max']\n",
    "  train['installments_ratio'] = train['new_installments_sum']/train['hist_installments_sum']\n",
    "  train['price_total'] = train['purchase_amount_total'] / train['installments_total']\n",
    "  train['price_mean'] = train['purchase_amount_mean'] / train['installments_mean']\n",
    "  train['price_max'] = train['purchase_amount_max'] / train['installments_max']\n",
    "  train['duration_mean'] = train['new_duration_mean']+train['hist_duration_mean']\n",
    "  train['duration_min'] = train['new_duration_min']+train['hist_duration_min']\n",
    "  train['duration_max'] = train['new_duration_max']+train['hist_duration_max']\n",
    "  train['amount_month_ratio_mean']=train['new_amount_month_ratio_mean']+train['hist_amount_month_ratio_mean']\n",
    "  train['amount_month_ratio_min']=train['new_amount_month_ratio_min']+train['hist_amount_month_ratio_min']\n",
    "  train['amount_month_ratio_max']=train['new_amount_month_ratio_max']+train['hist_amount_month_ratio_max']\n",
    "  train['new_CLV'] = train['new_card_id_count'] * train['new_purchase_amount_sum'] / train['new_month_diff_mean']\n",
    "  train['hist_CLV'] = train['hist_card_id_count'] * train['hist_purchase_amount_sum'] / train['hist_month_diff_mean']\n",
    "  train['CLV_ratio'] = train['new_CLV'] / train['hist_CLV']\n",
    "  gc.collect()\n",
    "\n",
    "  # Mean encoding\n",
    "  print(\"train\", train.shape)\n",
    "  print('Mean encoding...')\n",
    "  #Mean encoding new_merchants_transactions\n",
    "  # Loading data\n",
    "  merchants = reduce_mem_usage(pd.read_csv('/content/merchants.csv'))\n",
    "  merchants.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "  \n",
    "  new_transactions = reduce_mem_usage(pd.read_csv('/content/new_merchant_transactions.csv'))\n",
    "  new_transactions.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "\n",
    "  # Mean encoding\n",
    "  new_trans_mean = mean_encode(train, new_transactions, merchants, suffix='_new')\n",
    "  print('new mean encoding Done')\n",
    "  train = pd.merge(train, new_trans_mean, on='card_id',how='left')\n",
    "  del new_trans_mean\n",
    "  print(\"train\", train.shape)\n",
    "  # Mean encoding hist_merchants_transactions\n",
    "  # Loading data\n",
    "  merchants = reduce_mem_usage(pd.read_csv('/content/merchants.csv'))\n",
    "  merchants.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "  hist_transactions = reduce_mem_usage(pd.read_csv('../content/historical_transactions.csv'))\n",
    "  hist_transactions.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "  \n",
    "  # Mean encoding\n",
    "  hist_trans_mean = mean_encode(train, hist_transactions, merchants, suffix='_hist')\n",
    "  print('hist mean encoding Done')\n",
    "  train = pd.merge(train, hist_trans_mean, on='card_id',how='left')\n",
    "  del hist_trans_mean\n",
    "\n",
    "  train_col = ['authorized_flag_new', 'category_1_new', 'month_lag_new', 'installments_new','category_2_new',\n",
    "          'numerical_1_merchants_new', 'category_1_merchants_new', 'most_recent_sales_range_merchants_new',\n",
    "          'most_recent_purchases_range_merchants_new', 'active_months_lag3_merchants_new', \n",
    "          'active_months_lag6_merchants_new', 'active_months_lag12_merchants_new', 'category_4_merchants_new',\n",
    "          'category_2_merchants_new', \n",
    "          'authorized_flag_hist', 'category_1_hist', 'month_lag_hist', 'installments_hist', 'category_2_hist',\n",
    "          'numerical_1_merchants_hist', 'category_1_merchants_hist', 'most_recent_sales_range_merchants_hist',\n",
    "          'most_recent_purchases_range_merchants_hist', 'active_months_lag3_merchants_hist', \n",
    "          'active_months_lag6_merchants_hist', 'active_months_lag12_merchants_hist', 'category_4_merchants_hist',\n",
    "          'category_2_merchants_hist']\n",
    "\n",
    "  for col in train.columns:\n",
    "      if train[col].isna().any():\n",
    "          train[col].replace([-np.inf, np.inf, np.nan], train[col].min(), inplace=True)\n",
    "    \n",
    "  #deleting basic columns\n",
    "  del_col = ['feature_1','feature_2','feature_3', 'authorized_flag_new', 'merchant_category_id_new',\n",
    "       'subsector_id_new', 'category_1_new', 'month_lag_new',\n",
    "       'installments_new', 'purchase_amount_new', 'city_id_new',\n",
    "       'state_id_new', 'category_2_new', 'purchase_date_new',\n",
    "       'merchant_group_id_merchants_new', 'merchant_category_id_merchants_new',\n",
    "       'subsector_id_merchants_new', 'numerical_1_merchants_new',\n",
    "       'numerical_2_merchants_new', 'category_1_merchants_new',\n",
    "       'most_recent_sales_range_merchants_new',\n",
    "       'most_recent_purchases_range_merchants_new',\n",
    "       'avg_sales_lag3_merchants_new', 'avg_purchases_lag3_merchants_new',\n",
    "       'active_months_lag3_merchants_new', 'avg_sales_lag6_merchants_new',\n",
    "       'avg_purchases_lag6_merchants_new', 'active_months_lag6_merchants_new',\n",
    "       'avg_sales_lag12_merchants_new', 'avg_purchases_lag12_merchants_new',\n",
    "       'active_months_lag12_merchants_new', 'category_4_merchants_new',\n",
    "       'city_id_merchants_new', 'state_id_merchants_new',\n",
    "       'category_2_merchants_new', 'authorized_flag_hist',\n",
    "       'merchant_category_id_hist', 'subsector_id_hist', 'category_1_hist',\n",
    "       'month_lag_hist', 'installments_hist', 'purchase_amount_hist',\n",
    "       'city_id_hist', 'state_id_hist', 'category_2_hist',\n",
    "       'purchase_date_hist', 'merchant_group_id_merchants_hist',\n",
    "       'merchant_category_id_merchants_hist', 'subsector_id_merchants_hist',\n",
    "       'numerical_1_merchants_hist', 'numerical_2_merchants_hist',\n",
    "       'category_1_merchants_hist', 'most_recent_sales_range_merchants_hist',\n",
    "       'most_recent_purchases_range_merchants_hist',\n",
    "       'avg_sales_lag3_merchants_hist', 'avg_purchases_lag3_merchants_hist',\n",
    "       'active_months_lag3_merchants_hist', 'avg_sales_lag6_merchants_hist',\n",
    "       'avg_purchases_lag6_merchants_hist',\n",
    "       'active_months_lag6_merchants_hist', 'avg_sales_lag12_merchants_hist',\n",
    "       'avg_purchases_lag12_merchants_hist',\n",
    "       'active_months_lag12_merchants_hist', 'category_4_merchants_hist',\n",
    "       'city_id_merchants_hist', 'state_id_merchants_hist',\n",
    "       'category_2_merchants_hist']\n",
    "  train = train.drop(del_col, axis = 1)\n",
    "  train = train.loc[:,train.columns[0:200]]\n",
    "  Exclude = ['first_active_month', 'target','merchant_id', 'card_id', 'outliers',\n",
    "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size']\n",
    "  df_train_columns = [c for c in train.columns if c not in Exclude] \n",
    "  train = train[df_train_columns]\n",
    "  train.head()\n",
    "  #train.to_pickle(\"/content/train_men.pkl\")\n",
    "  return train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uAkBOmfRG8Fz"
   },
   "source": [
    "## final_function_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9Y3Sg8sO1Nw"
   },
   "outputs": [],
   "source": [
    "def final_fun_1(train, transactions = None, new_transactions = None, merchants = None):\n",
    "    #Preprocessing train and test data\n",
    "    train= getfeatures_train(train, transactions = None, new_transactions = None, merchants = None)\n",
    "    filename = '/content/drive/My Drive/Colab Notebooks/CS1/pkl_old/pkl/lgbm1.sav'\n",
    "    lgbm = pd.read_pickle(filename)\n",
    "    train_predictions = lgbm.predict(train)\n",
    "    \n",
    "    return train_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ahIiP4i8pXw"
   },
   "source": [
    "## final_function_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGmaoHW3Gc02"
   },
   "outputs": [],
   "source": [
    "def final_fun_2(train, target, transactions = None, new_transactions = None, merchants = None):\n",
    "    train_predictions = final_fun_1(train, transactions = None, new_transactions = None, merchants = None)\n",
    "    print(\"\\n Actual value:\", target)\n",
    "    print(\"\\n predicted value:\", train_predictions)\n",
    "    print(\"\\n Root mean squared error: {:<8.5f}\".format(mean_squared_error(train_predictions, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RQleuaMwdDBK",
    "outputId": "4170e5a7-1f3e-4cc9-9092-120e87258554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(pd.read_csv('/content/train.csv',parse_dates=[\"first_active_month\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2InFx_636Sb"
   },
   "source": [
    "### Metrics: RMSE of single data point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "awgYIX5eY6AZ",
    "outputId": "c17d8e35-eeb7-4d91-98a7-3b62229f61a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n",
      "Preprocessing hist_trans...\n",
      "Mem. usage decreased to 129.17 Mb (42.5% reduction)\n",
      "Preprocessing new_trans...\n",
      "train (1, 199)\n",
      "Mean encoding...\n",
      "Mem. usage decreased to 30.32 Mb (46.0% reduction)\n",
      "Mem. usage decreased to 114.20 Mb (45.5% reduction)\n",
      "new mean encoding Done\n",
      "train (1, 245)\n",
      "Mem. usage decreased to 30.32 Mb (46.0% reduction)\n",
      "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n",
      "hist mean encoding Done\n",
      "\n",
      " Actual value: 0   -0.820312\n",
      "Name: target, dtype: float16\n",
      "\n",
      " predicted value: [-0.25214436]\n",
      "\n",
      " Root mean squared error: 0.56817 \n",
      "CPU times: user 35min 22s, sys: 52 s, total: 36min 14s\n",
      "Wall time: 36min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_fun_2(train[0:1], train['target'][0:1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
